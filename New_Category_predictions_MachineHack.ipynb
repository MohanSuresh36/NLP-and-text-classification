{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "Text Classification - Machine Learning and Basic DNN Models.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohanSuresh36/NLP-and-text-classification/blob/main/New_Category_predictions_MachineHack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLOatR1LRaE_"
      },
      "source": [
        "# Sentiment Analysis - Machine Learning and Basic Deep Neural Network Models\n",
        "\n",
        "We have already discussed that sentiment analysis, also popularly known as opinion analysis or opinion mining is one of the most important applications of NLP. The key idea is to predict the potential sentiment of a body of text based on the textual content. In this sub-unit, we will be exploring supervised learning models. \n",
        "\n",
        "![](https://github.com/dipanjanS/nlp_workshop_dhs18/blob/master/Unit%2012%20-%20Project%209%20-%20Sentiment%20Analysis%20-%20Supervised%20Learning/sentiment_cover.png?raw=1)\n",
        "\n",
        "Another way to build a model to understand the text content and predict the sentiment of the text based reviews is to use supervised machine learning. To be more specific, we will be using classification models for solving this problem. We will be building an automated sentiment text classification system in subsequent sections. The major steps to achieve this are mentioned as follows.\n",
        "\n",
        "1.\tPrepare train and test datasets (optionally a validation dataset)\n",
        "2.\tPre-process and normalize text documents\n",
        "3.\tFeature Engineering \n",
        "4.\tModel training\n",
        "5.\tModel prediction and evaluation\n",
        "\n",
        "These are the major steps for building our system. Optionally the last step would be to deploy the model in your server or on the cloud. The following figure shows a detailed workflow for building a standard text classification system with supervised learning (classification) models.\n",
        "\n",
        "![](https://github.com/dipanjanS/nlp_workshop_dhs18/blob/master/Unit%2012%20-%20Project%209%20-%20Sentiment%20Analysis%20-%20Supervised%20Learning/sentiment_classifier_workflow.png?raw=1)\n",
        "\n",
        "\n",
        "In our scenario, documents indicate the movie reviews and classes indicate the review sentiments which can either be positive or negative making it a binary classification problem. We will build models using both traditional machine learning methods and newer deep learning in the subsequent sections. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "610bNp_4SQma",
        "outputId": "7f5b994e-802f-43cb-cffa-20100463b199"
      },
      "source": [
        "!pip install contractions\n",
        "!pip install textsearch\n",
        "!pip install tqdm\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/04/d5e0bb9f2cef5d15616ebf68087a725c5dbdd71bd422bcfb35d709f98ce7/contractions-0.0.48-py2.py3-none-any.whl\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/c2/eae730037ae1cbbfaa229d27030d1d5e34a1e41114b21447d1202ae9c220/pyahocorasick-1.4.2.tar.gz (321kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 27.9MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/c7/61370d9e3c349478e89a5554c1e5d9658e1e3116cc4f2528f568909ebdf1/anyascii-0.1.7-py3-none-any.whl (260kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 51.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85399 sha256=6f15d3eca7cb55243297d71b433105e3d5bbf368cd13d8ad8e2759add515fb21\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.1.7 contractions-0.0.48 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.7/dist-packages (0.0.21)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch) (1.4.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch) (0.1.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMFvLtEHRaFM"
      },
      "source": [
        "# Load and View Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgphrYufRaFR",
        "outputId": "a321a536-dd8c-4ae9-d339-38b64a09022f"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_excel('/content/Data_Train.xlsx')\n",
        "test_dataset = pd.read_excel('/content/Data_Test.xlsx')\n",
        "dataset.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7628 entries, 0 to 7627\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   STORY    7628 non-null   object\n",
            " 1   SECTION  7628 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 119.3+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "5nkEEGExRaFc",
        "outputId": "3ffb74c7-913f-4eb7-d416-4188ca93d440"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>STORY</th>\n",
              "      <th>SECTION</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>But the most painful was the huge reversal in ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How formidable is the opposition alliance amon...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Most Asian currencies were trading lower today...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If you want to answer any question, click on ‘...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In global markets, gold prices edged up today ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               STORY  SECTION\n",
              "0  But the most painful was the huge reversal in ...        3\n",
              "1  How formidable is the opposition alliance amon...        0\n",
              "2  Most Asian currencies were trading lower today...        3\n",
              "3  If you want to answer any question, click on ‘...        1\n",
              "4  In global markets, gold prices edged up today ...        3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V1NWGhcRaFi"
      },
      "source": [
        "# Build Train and Test Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JP10IEYRaFj"
      },
      "source": [
        "# build train and test datasets\n",
        "sentiments= dataset['SECTION'].values\n",
        "reviews = dataset['STORY'].values\n",
        "\n",
        "train_reviews = dataset['STORY']\n",
        "train_sentiments = dataset['SECTION']\n",
        "\n",
        "test_reviews = test_dataset['STORY']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCDcSNTjmgYn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5564df99-8aca-444a-c13e-202ce4fb3bf7"
      },
      "source": [
        "to_categorical(train_sentiments)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [0., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 1., 0., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i45AFxXNRaFn"
      },
      "source": [
        "# Text Wrangling & Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHZ0lEGNRaFo"
      },
      "source": [
        "import contractions\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import re\n",
        "import tqdm\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "def strip_html_tags(text):\n",
        "  soup = BeautifulSoup(text, \"html.parser\")\n",
        "  [s.extract() for s in soup(['iframe', 'script'])]\n",
        "  stripped_text = soup.get_text()\n",
        "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "  return stripped_text\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "  return text\n",
        "\n",
        "def pre_process_corpus(docs):\n",
        "  norm_docs = []\n",
        "  for doc in tqdm.tqdm(docs):\n",
        "    doc = strip_html_tags(doc)\n",
        "    doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    doc = doc.lower()\n",
        "    doc = remove_accented_chars(doc)\n",
        "    doc = contractions.fix(doc)\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
        "    doc = re.sub(' +', ' ', doc)\n",
        "    doc = doc.strip()  \n",
        "    norm_docs.append(doc)\n",
        "  \n",
        "  return norm_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO3ESug2RaFr",
        "outputId": "37ca87f2-1cf9-4535-bcea-97d1ba3c0fba"
      },
      "source": [
        "%%time\n",
        "\n",
        "norm_train_reviews = pre_process_corpus(train_reviews)\n",
        "norm_test_reviews = pre_process_corpus(test_reviews)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7628/7628 [00:01<00:00, 4132.47it/s]\n",
            "100%|██████████| 2748/2748 [00:00<00:00, 4114.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2.46 s, sys: 49.9 ms, total: 2.51 s\n",
            "Wall time: 2.52 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucDv8n50RaFu"
      },
      "source": [
        "# Traditional Supervised Machine Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOZ7Rn0jRaFv"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD8q5QoERaFw",
        "outputId": "350665f7-e7a8-42b9-ce33-144dbbd5157d"
      },
      "source": [
        "%%time\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# build BOW features on train reviews\n",
        "cv = CountVectorizer(binary=False, min_df=5, max_df=1.0, ngram_range=(1,2))\n",
        "cv_train_features = cv.fit_transform(norm_train_reviews)\n",
        "\n",
        "\n",
        "# build TFIDF features on train reviews\n",
        "tv = TfidfVectorizer(use_idf=True, min_df=5, max_df=1.0, ngram_range=(1,2),\n",
        "                     sublinear_tf=True)\n",
        "tv_train_features = tv.fit_transform(norm_train_reviews)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.5 s, sys: 158 ms, total: 4.65 s\n",
            "Wall time: 4.66 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSvqpHRYRaFz",
        "outputId": "1899c7ec-c59c-475d-bd42-3beefe8974f3"
      },
      "source": [
        "%%time\n",
        "\n",
        "# transform test reviews into features\n",
        "cv_test_features = cv.transform(norm_test_reviews)\n",
        "tv_test_features = tv.transform(norm_test_reviews)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 847 ms, sys: 2.56 ms, total: 849 ms\n",
            "Wall time: 850 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfQPYw8PRaF2",
        "outputId": "b91f88e2-8570-4788-eaf5-2610dcefb85a"
      },
      "source": [
        "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\n",
        "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOW model:> Train features shape: (7628, 31941)  Test features shape: (2748, 31941)\n",
            "TFIDF model:> Train features shape: (7628, 31941)  Test features shape: (2748, 31941)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aUUsxMrRaF7"
      },
      "source": [
        "## Model Training, Prediction and Performance Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPA5UtYFRaF8"
      },
      "source": [
        "### Try out Logistic Regression\n",
        "\n",
        "The logistic regression model is actually a statistical model developed by statistician\n",
        "David Cox in 1958. It is also known as the logit or logistic model since it uses the\n",
        "logistic (popularly also known as sigmoid) mathematical function to estimate the\n",
        "parameter values. These are the coefficients of all our features such that the overall loss\n",
        "is minimized when predicting the outcome—"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "JGHQErnMRaF9",
        "outputId": "7a94f0a8-26f0-4dac-81ce-ef84a8f74af4"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Logistic Regression model on BOW features\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# instantiate model\n",
        "lr = LogisticRegression(penalty='l2', max_iter=500, C=1, solver='lbfgs', random_state=42)\n",
        "\n",
        "# train model\n",
        "lr.fit(cv_train_features, train_sentiments)\n",
        "\n",
        "# predict on test data\n",
        "lr_bow_predictions = lr.predict(cv_test_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4min 37s, sys: 1min 40s, total: 6min 18s\n",
            "Wall time: 1min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "wqeRyayrRaGA",
        "outputId": "c1a86261-88d3-4b8b-b354-97cc5a54f75c"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, lr_bow_predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, lr_bow_predictions), index=labels, columns=labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.90      0.90      0.90      7490\n",
            "    positive       0.90      0.91      0.90      7510\n",
            "\n",
            "    accuracy                           0.90     15000\n",
            "   macro avg       0.90      0.90      0.90     15000\n",
            "weighted avg       0.90      0.90      0.90     15000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>6754</td>\n",
              "      <td>736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>711</td>\n",
              "      <td>6799</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative      6754       736\n",
              "positive       711      6799"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "uQEAz6O6RaGC",
        "outputId": "4a280237-1323-4458-f021-696e77457cde"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Logistic Regression model on TF-IDF features\n",
        "\n",
        "# train model\n",
        "lr.fit(tv_train_features, train_sentiments)\n",
        "\n",
        "# predict on test data\n",
        "lr_tfidf_predictions = lr.predict(tv_test_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 11.6 s, sys: 4.67 s, total: 16.2 s\n",
            "Wall time: 2.84 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "HNCfZnUORaGE",
        "outputId": "0a4f17bf-1d18-48c8-c998-eeba611ebcb8"
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, lr_tfidf_predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, lr_tfidf_predictions), index=labels, columns=labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.91      0.89      0.90      7490\n",
            "    positive       0.90      0.91      0.90      7510\n",
            "\n",
            "    accuracy                           0.90     15000\n",
            "   macro avg       0.90      0.90      0.90     15000\n",
            "weighted avg       0.90      0.90      0.90     15000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>6694</td>\n",
              "      <td>796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>665</td>\n",
              "      <td>6845</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative      6694       796\n",
              "positive       665      6845"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoOoEAiXRaGH"
      },
      "source": [
        "### Try out Random Forest\n",
        "\n",
        "Decision trees are a family of supervised machine learning algorithms that can represent\n",
        "and interpret sets of rules automatically from the underlying data. They use metrics like\n",
        "information gain and gini-index to build the tree. However, a major drawback of decision\n",
        "trees is that since they are non-parametric, the more data there is, greater the depth of\n",
        "the tree. We can end up with really huge and deep trees that are prone to overfitting. The\n",
        "model might work really well on training data, but instead of learning, it just memorizes\n",
        "all the training samples and builds very specific rules to them. Hence, it performs really\n",
        "poorly on the test data. Random forests try to tackle this problem.\n",
        "\n",
        "A random forest is a meta-estimator or an ensemble model that fits a number of\n",
        "decision tree classifiers on various sub-samples of the dataset and uses averaging to\n",
        "improve the predictive accuracy and control over-fitting. The sub-sample size is always\n",
        "the same as the original input sample size, but the samples are drawn with replacement\n",
        "(bootstrap samples). In random forests, all the trees are trained in parallel (bagging\n",
        "model/bootstrap aggregation). Besides this, each tree in the ensemble is built from a\n",
        "sample drawn with replacement (i.e., a bootstrap sample) from the training set. Also,\n",
        "when splitting a node during the construction of the tree, the split that is chosen is no\n",
        "longer the best split among all features. Instead, the split that is picked is the best split\n",
        "among a random subset of the features. T"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "YzaSSOpYRaGH",
        "outputId": "98e50c3d-c50f-4757-aceb-5c19f67b7bee"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Random Forest model on BOW features\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# instantiate model\n",
        "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
        "\n",
        "# train model\n",
        "rf.fit(cv_train_features, train_sentiments)\n",
        "\n",
        "# predict on test data\n",
        "rf_bow_predictions = rf.predict(cv_test_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 33s, sys: 1.18 s, total: 3min 34s\n",
            "Wall time: 31.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "617Kuv7_RaGJ",
        "outputId": "f21b91c0-1a7d-467c-c865-5398189a1f3f"
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, rf_bow_predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, rf_bow_predictions), index=labels, columns=labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.86      0.86      0.86      7490\n",
            "    positive       0.86      0.86      0.86      7510\n",
            "\n",
            "    accuracy                           0.86     15000\n",
            "   macro avg       0.86      0.86      0.86     15000\n",
            "weighted avg       0.86      0.86      0.86     15000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>6406</td>\n",
              "      <td>1084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>1080</td>\n",
              "      <td>6430</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative      6406      1084\n",
              "positive      1080      6430"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "WdvmBOrPRaGM",
        "outputId": "7b9bac4d-d4c4-40cb-d1c5-6ed649443d6f"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Random Forest model on TF-IDF features\n",
        "\n",
        "# train model\n",
        "rf.fit(tv_train_features, train_sentiments)\n",
        "\n",
        "# predict on test data\n",
        "rf_tfidf_predictions = rf.predict(tv_test_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 21s, sys: 1.02 s, total: 3min 22s\n",
            "Wall time: 30.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "8hBOMh6uRaGP",
        "outputId": "f36e34e4-2133-44f6-90e2-306db219a69e"
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, rf_tfidf_predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, rf_tfidf_predictions), index=labels, columns=labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.85      0.86      0.85      7490\n",
            "    positive       0.86      0.84      0.85      7510\n",
            "\n",
            "    accuracy                           0.85     15000\n",
            "   macro avg       0.85      0.85      0.85     15000\n",
            "weighted avg       0.85      0.85      0.85     15000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>6458</td>\n",
              "      <td>1032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>1175</td>\n",
              "      <td>6335</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative      6458      1032\n",
              "positive      1175      6335"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpEum7R9kcxu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoqZhMQFRaGS"
      },
      "source": [
        "# Newer Supervised Deep Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZw6LYNHRaGT"
      },
      "source": [
        "import gensim\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Activation, Dense\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiZbcv_gRaGZ"
      },
      "source": [
        "## Prediction class label encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnhC4rWaRaGb"
      },
      "source": [
        "# converting y data into categorical (one-hot encoding)\n",
        "y_train = to_categorical(train_sentiments)\n",
        "#y_test = to_categorical(y_test)\n",
        "\n",
        "# tokenize train reviews & encode train labels\n",
        "tokenized_train = [nltk.word_tokenize(text)\n",
        "                       for text in norm_train_reviews]\n",
        "y_train = le.fit_transform(y_train)\n",
        "# tokenize test reviews & encode test labels\n",
        "tokenized_test = [nltk.word_tokenize(text)\n",
        "                      for text in norm_test_reviews]\n",
        "#y_test = le.fit_transform(test_sentiments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "4ogDRDh4RaGg",
        "outputId": "0f0906e2-ba4d-411a-e69e-15b6161a7953"
      },
      "source": [
        "# print class label encoding map and encoded labels\n",
        "print('Sentiment class label map:', dict(zip(le.classes_, le.transform(le.classes_))))\n",
        "print('Sample test label transformation:\\n'+'-'*35,\n",
        "      '\\nActual Labels:', test_sentiments[:3], '\\nEncoded Labels:', y_test[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-633ee7605d6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print class label encoding map and encoded labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sentiment class label map:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m print('Sample test label transformation:\\n'+'-'*35,\n\u001b[1;32m      4\u001b[0m       '\\nActual Labels:', test_sentiments[:3], '\\nEncoded Labels:', y_test[:3])\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'OneHotEncoder' object has no attribute 'classes_'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdexbrYXRaGk"
      },
      "source": [
        "## Feature Engineering with word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5S0u0BbiN2a"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9kfCw6LRaGl",
        "outputId": "fb3b8466-f4eb-475d-91e5-f37658777efd"
      },
      "source": [
        "%%time\n",
        "# build word2vec model\n",
        "w2v_num_features = 300\n",
        "w2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features, window=150,\n",
        "                                   min_count=10, workers=4, iter=5)    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-11 06:18:06,632 : INFO : collecting all words and their counts\n",
            "2021-04-11 06:18:06,634 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2021-04-11 06:18:06,791 : INFO : collected 40121 word types from a corpus of 821529 raw words and 7628 sentences\n",
            "2021-04-11 06:18:06,792 : INFO : Loading a fresh vocabulary\n",
            "2021-04-11 06:18:06,829 : INFO : effective_min_count=10 retains 6846 unique words (17% of original 40121, drops 33275)\n",
            "2021-04-11 06:18:06,830 : INFO : effective_min_count=10 leaves 746491 word corpus (90% of original 821529, drops 75038)\n",
            "2021-04-11 06:18:06,852 : INFO : deleting the raw counts dictionary of 40121 items\n",
            "2021-04-11 06:18:06,854 : INFO : sample=0.001 downsamples 36 most-common words\n",
            "2021-04-11 06:18:06,856 : INFO : downsampling leaves estimated 572095 word corpus (76.6% of prior 746491)\n",
            "2021-04-11 06:18:06,872 : INFO : estimated required memory for 6846 words and 300 dimensions: 19853400 bytes\n",
            "2021-04-11 06:18:06,873 : INFO : resetting layer weights\n",
            "2021-04-11 06:18:08,112 : INFO : training model with 4 workers on 6846 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=150\n",
            "2021-04-11 06:18:09,125 : INFO : EPOCH 1 - PROGRESS: at 22.43% examples, 123714 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 06:18:10,200 : INFO : EPOCH 1 - PROGRESS: at 46.12% examples, 126522 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 06:18:11,234 : INFO : EPOCH 1 - PROGRESS: at 70.11% examples, 128593 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 06:18:12,243 : INFO : EPOCH 1 - PROGRESS: at 94.06% examples, 130615 words/s, in_qsize 5, out_qsize 0\n",
            "2021-04-11 06:18:12,279 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-04-11 06:18:12,284 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-11 06:18:12,365 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-11 06:18:12,370 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-11 06:18:12,371 : INFO : EPOCH - 1 : training on 821529 raw words (572349 effective words) took 4.3s, 134614 effective words/s\n",
            "2021-04-11 06:18:13,416 : INFO : EPOCH 2 - PROGRESS: at 23.74% examples, 126942 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 06:18:14,448 : INFO : EPOCH 2 - PROGRESS: at 47.36% examples, 130708 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 06:18:15,458 : INFO : EPOCH 2 - PROGRESS: at 71.38% examples, 132429 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 06:18:16,477 : INFO : EPOCH 2 - PROGRESS: at 94.06% examples, 131467 words/s, in_qsize 5, out_qsize 0\n",
            "2021-04-11 06:18:16,502 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-04-11 06:18:16,504 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-11 06:18:16,593 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-11 06:18:16,597 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-11 06:18:16,598 : INFO : EPOCH - 2 : training on 821529 raw words (571982 effective words) took 4.2s, 135688 effective words/s\n",
            "2021-04-11 06:18:17,613 : INFO : EPOCH 3 - PROGRESS: at 22.43% examples, 124013 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 06:18:18,701 : INFO : EPOCH 3 - PROGRESS: at 46.12% examples, 125522 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 06:18:19,727 : INFO : EPOCH 3 - PROGRESS: at 70.11% examples, 128357 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 06:18:20,793 : INFO : EPOCH 3 - PROGRESS: at 95.20% examples, 130166 words/s, in_qsize 4, out_qsize 0\n",
            "2021-04-11 06:18:20,806 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-04-11 06:18:20,819 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-11 06:18:20,850 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-11 06:18:20,879 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-11 06:18:20,880 : INFO : EPOCH - 3 : training on 821529 raw words (571709 effective words) took 4.3s, 133866 effective words/s\n",
            "2021-04-11 06:18:21,946 : INFO : EPOCH 4 - PROGRESS: at 23.74% examples, 123989 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 06:18:22,949 : INFO : EPOCH 4 - PROGRESS: at 46.12% examples, 127327 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 06:18:23,965 : INFO : EPOCH 4 - PROGRESS: at 69.06% examples, 127886 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 06:18:24,973 : INFO : EPOCH 4 - PROGRESS: at 94.06% examples, 131741 words/s, in_qsize 5, out_qsize 0\n",
            "2021-04-11 06:18:25,099 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-04-11 06:18:25,116 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-11 06:18:25,146 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-11 06:18:25,174 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-11 06:18:25,175 : INFO : EPOCH - 4 : training on 821529 raw words (571952 effective words) took 4.3s, 133394 effective words/s\n",
            "2021-04-11 06:18:26,212 : INFO : EPOCH 5 - PROGRESS: at 23.74% examples, 127085 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 06:18:27,247 : INFO : EPOCH 5 - PROGRESS: at 47.36% examples, 130409 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 06:18:28,264 : INFO : EPOCH 5 - PROGRESS: at 71.38% examples, 132038 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 06:18:29,270 : INFO : EPOCH 5 - PROGRESS: at 95.20% examples, 133193 words/s, in_qsize 4, out_qsize 0\n",
            "2021-04-11 06:18:29,305 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-04-11 06:18:29,345 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-11 06:18:29,383 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-11 06:18:29,389 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-11 06:18:29,392 : INFO : EPOCH - 5 : training on 821529 raw words (571835 effective words) took 4.2s, 135814 effective words/s\n",
            "2021-04-11 06:18:29,395 : INFO : training on a 4107645 raw words (2859827 effective words) took 21.3s, 134378 effective words/s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 42.9 s, sys: 113 ms, total: 43 s\n",
            "Wall time: 22.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUZHFsj8RaGo"
      },
      "source": [
        "def averaged_word2vec_vectorizer(corpus, model, num_features):\n",
        "    vocabulary = set(model.wv.index2word)\n",
        "    \n",
        "    def average_word_vectors(words, model, vocabulary, num_features):\n",
        "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
        "        nwords = 0.\n",
        "        \n",
        "        for word in words:\n",
        "            if word in vocabulary: \n",
        "                nwords = nwords + 1.\n",
        "                feature_vector = np.add(feature_vector, model.wv[word])\n",
        "        if nwords:\n",
        "            feature_vector = np.divide(feature_vector, nwords)\n",
        "\n",
        "        return feature_vector\n",
        "\n",
        "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
        "                    for tokenized_sentence in corpus]\n",
        "    return np.array(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWfcUVixRaGr"
      },
      "source": [
        "# generate averaged word vector features from word2vec model\n",
        "avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model,\n",
        "                                                     num_features=w2v_num_features)\n",
        "avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
        "                                                    num_features=w2v_num_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GMe1dCfRaGw",
        "outputId": "a9e17c58-efb0-490a-e2b5-24988fcb23d7"
      },
      "source": [
        "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec model:> Train features shape: (7628, 300)  Test features shape: (2748, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mSFQ9H0RaGy"
      },
      "source": [
        "## Modeling with deep neural networks "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oz9gtU9RaGz"
      },
      "source": [
        "### Building Deep neural network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSHktNs0nvVe"
      },
      "source": [
        "from keras.layers import BatchNormalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCI0uEBIRaGz"
      },
      "source": [
        "def construct_deepnn_architecture(num_input_features):\n",
        "    dnn_model = Sequential()\n",
        "    dnn_model.add(Dense(512, input_shape=(num_input_features,), kernel_initializer='he_normal'))\n",
        "    dnn_model.add(BatchNormalization())\n",
        "    dnn_model.add(Activation('elu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(256, kernel_initializer='he_normal'))\n",
        "    dnn_model.add(BatchNormalization())\n",
        "    dnn_model.add(Activation('elu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(256, kernel_initializer='he_normal'))\n",
        "    dnn_model.add(BatchNormalization())\n",
        "    dnn_model.add(Activation('elu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(4))\n",
        "    dnn_model.add(Activation('softmax'))\n",
        "\n",
        "    dnn_model.compile(loss='categorical_crossentropy', optimizer='adam',                 \n",
        "                      metrics=['accuracy'])\n",
        "    return dnn_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cQpTH6FRaG1"
      },
      "source": [
        "w2v_dnn = construct_deepnn_architecture(num_input_features=w2v_num_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBzbo-YlRaG4"
      },
      "source": [
        "### Visualize sample deep architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fuhzYagRaG5",
        "outputId": "28c2f703-6610-406b-d6b7-029ca2cb378a"
      },
      "source": [
        "w2v_dnn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_8 (Dense)              (None, 512)               154112    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 4)                 1028      \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 4)                 0         \n",
            "=================================================================\n",
            "Total params: 356,356\n",
            "Trainable params: 354,308\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-COslmPRaG8"
      },
      "source": [
        "### Model Training, Prediction and Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kcisAskRaG8",
        "outputId": "fb268b86-447f-434e-93b0-368a6cf08327"
      },
      "source": [
        "batch_size = 100\n",
        "w2v_dnn.fit(avg_wv_train_features, to_categorical(train_sentiments), epochs=50, batch_size=batch_size, \n",
        "            shuffle=True, validation_split=0.1, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "69/69 [==============================] - 1s 7ms/step - loss: 0.4120 - accuracy: 0.8440 - val_loss: 0.1959 - val_accuracy: 0.9423\n",
            "Epoch 2/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.1737 - accuracy: 0.9425 - val_loss: 0.1458 - val_accuracy: 0.9528\n",
            "Epoch 3/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.1658 - accuracy: 0.9416 - val_loss: 0.1563 - val_accuracy: 0.9476\n",
            "Epoch 4/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.1590 - accuracy: 0.9503 - val_loss: 0.1337 - val_accuracy: 0.9620\n",
            "Epoch 5/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.1378 - accuracy: 0.9519 - val_loss: 0.1270 - val_accuracy: 0.9581\n",
            "Epoch 6/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.1385 - accuracy: 0.9485 - val_loss: 0.1296 - val_accuracy: 0.9581\n",
            "Epoch 7/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.1328 - accuracy: 0.9526 - val_loss: 0.1431 - val_accuracy: 0.9554\n",
            "Epoch 8/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.1303 - accuracy: 0.9552 - val_loss: 0.1402 - val_accuracy: 0.9515\n",
            "Epoch 9/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.1308 - accuracy: 0.9544 - val_loss: 0.1427 - val_accuracy: 0.9489\n",
            "Epoch 10/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.1133 - accuracy: 0.9601 - val_loss: 0.1210 - val_accuracy: 0.9620\n",
            "Epoch 11/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.1182 - accuracy: 0.9558 - val_loss: 0.1428 - val_accuracy: 0.9502\n",
            "Epoch 12/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.1256 - accuracy: 0.9539 - val_loss: 0.1216 - val_accuracy: 0.9581\n",
            "Epoch 13/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.1169 - accuracy: 0.9574 - val_loss: 0.1306 - val_accuracy: 0.9607\n",
            "Epoch 14/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.1113 - accuracy: 0.9608 - val_loss: 0.1327 - val_accuracy: 0.9567\n",
            "Epoch 15/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.1122 - accuracy: 0.9616 - val_loss: 0.1344 - val_accuracy: 0.9541\n",
            "Epoch 16/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.1061 - accuracy: 0.9612 - val_loss: 0.1348 - val_accuracy: 0.9528\n",
            "Epoch 17/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.1018 - accuracy: 0.9647 - val_loss: 0.1470 - val_accuracy: 0.9528\n",
            "Epoch 18/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.1003 - accuracy: 0.9660 - val_loss: 0.1456 - val_accuracy: 0.9502\n",
            "Epoch 19/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.1010 - accuracy: 0.9648 - val_loss: 0.1261 - val_accuracy: 0.9581\n",
            "Epoch 20/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.1070 - accuracy: 0.9641 - val_loss: 0.1338 - val_accuracy: 0.9581\n",
            "Epoch 21/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.1066 - accuracy: 0.9615 - val_loss: 0.1364 - val_accuracy: 0.9620\n",
            "Epoch 22/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0873 - accuracy: 0.9679 - val_loss: 0.1278 - val_accuracy: 0.9581\n",
            "Epoch 23/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0964 - accuracy: 0.9658 - val_loss: 0.1195 - val_accuracy: 0.9567\n",
            "Epoch 24/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0929 - accuracy: 0.9645 - val_loss: 0.1217 - val_accuracy: 0.9633\n",
            "Epoch 25/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0887 - accuracy: 0.9644 - val_loss: 0.1238 - val_accuracy: 0.9607\n",
            "Epoch 26/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0789 - accuracy: 0.9691 - val_loss: 0.1245 - val_accuracy: 0.9594\n",
            "Epoch 27/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0825 - accuracy: 0.9702 - val_loss: 0.1305 - val_accuracy: 0.9594\n",
            "Epoch 28/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0786 - accuracy: 0.9708 - val_loss: 0.1364 - val_accuracy: 0.9581\n",
            "Epoch 29/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0748 - accuracy: 0.9726 - val_loss: 0.1457 - val_accuracy: 0.9489\n",
            "Epoch 30/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0805 - accuracy: 0.9719 - val_loss: 0.1295 - val_accuracy: 0.9594\n",
            "Epoch 31/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0733 - accuracy: 0.9711 - val_loss: 0.1545 - val_accuracy: 0.9528\n",
            "Epoch 32/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0775 - accuracy: 0.9716 - val_loss: 0.1454 - val_accuracy: 0.9607\n",
            "Epoch 33/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0809 - accuracy: 0.9703 - val_loss: 0.1329 - val_accuracy: 0.9554\n",
            "Epoch 34/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0768 - accuracy: 0.9694 - val_loss: 0.1375 - val_accuracy: 0.9528\n",
            "Epoch 35/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0738 - accuracy: 0.9733 - val_loss: 0.1382 - val_accuracy: 0.9541\n",
            "Epoch 36/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0832 - accuracy: 0.9681 - val_loss: 0.1472 - val_accuracy: 0.9541\n",
            "Epoch 37/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0711 - accuracy: 0.9742 - val_loss: 0.1491 - val_accuracy: 0.9541\n",
            "Epoch 38/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0806 - accuracy: 0.9663 - val_loss: 0.1663 - val_accuracy: 0.9476\n",
            "Epoch 39/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0709 - accuracy: 0.9728 - val_loss: 0.1349 - val_accuracy: 0.9620\n",
            "Epoch 40/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0615 - accuracy: 0.9774 - val_loss: 0.1414 - val_accuracy: 0.9528\n",
            "Epoch 41/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0741 - accuracy: 0.9709 - val_loss: 0.1380 - val_accuracy: 0.9541\n",
            "Epoch 42/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0593 - accuracy: 0.9803 - val_loss: 0.1369 - val_accuracy: 0.9633\n",
            "Epoch 43/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0685 - accuracy: 0.9728 - val_loss: 0.1576 - val_accuracy: 0.9567\n",
            "Epoch 44/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0610 - accuracy: 0.9781 - val_loss: 0.1374 - val_accuracy: 0.9567\n",
            "Epoch 45/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0637 - accuracy: 0.9786 - val_loss: 0.1537 - val_accuracy: 0.9476\n",
            "Epoch 46/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0616 - accuracy: 0.9753 - val_loss: 0.1333 - val_accuracy: 0.9607\n",
            "Epoch 47/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0632 - accuracy: 0.9769 - val_loss: 0.1564 - val_accuracy: 0.9554\n",
            "Epoch 48/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0686 - accuracy: 0.9735 - val_loss: 0.1490 - val_accuracy: 0.9554\n",
            "Epoch 49/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0580 - accuracy: 0.9776 - val_loss: 0.1737 - val_accuracy: 0.9515\n",
            "Epoch 50/50\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.0613 - accuracy: 0.9780 - val_loss: 0.1563 - val_accuracy: 0.9594\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f607fc30a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_8MFMh_mIjD"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8EUfWuumPc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1aab4ee-0897-40df-cd03-7956b93c071e"
      },
      "source": [
        "np.array(y_train )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(<7628x8 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 30512 stored elements in Compressed Sparse Row format>, dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C1gcc4iRaG-",
        "outputId": "7426ed09-f1aa-4dbc-a3a6-0dce966316ff"
      },
      "source": [
        "y_pred = w2v_dnn.predict_classes(avg_wv_test_features)\n",
        "y_pred\n",
        "#predictions =  np.argmax(y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CO0u3W36pOhG"
      },
      "source": [
        "submission = pd.read_excel('/content/Sample_submission.xlsx')\n",
        "submission['SECTION'] = y_pred\n",
        "submission.to_excel('/content/Sample_submission_w2v_CNN.xlsx',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saJzSivnpOnf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c44fb4d9-a576-48fe-ec40-090b646b003a"
      },
      "source": [
        "y_pred.max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "2XHEZ-X9RaG_",
        "outputId": "8bffc4eb-dec4-4308-bfbf-7fef5706512e"
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, predictions), index=labels, columns=labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.87      0.88      0.88      7490\n",
            "    positive       0.88      0.87      0.88      7510\n",
            "\n",
            "    accuracy                           0.88     15000\n",
            "   macro avg       0.88      0.88      0.88     15000\n",
            "weighted avg       0.88      0.88      0.88     15000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>6628</td>\n",
              "      <td>862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>984</td>\n",
              "      <td>6526</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative      6628       862\n",
              "positive       984      6526"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZxPWMlxscao"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "t = tf.keras.preprocessing.text.Tokenizer(oov_token='<UNK>')\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(norm_train_reviews)\n",
        "t.word_index['<PAD>'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjzXXXdpscX8"
      },
      "source": [
        "VOCAB_SIZE = len(t.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCRIuY4JscUY"
      },
      "source": [
        "train_sequences = t.texts_to_sequences(norm_train_reviews)\n",
        "test_sequences = t.texts_to_sequences(norm_test_reviews)\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=1000)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UloXbETmhMfy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f34be44-76b9-4162-ced3-bc76885dd50a"
      },
      "source": [
        "EMBEDDING_DIM = 300 # dimension for dense embeddings for each token\n",
        "LSTM_DIM = 128 # total LSTM units\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=1000))\n",
        "model.add(tf.keras.layers.SpatialDropout1D(0.1))\n",
        "model.add(tf.keras.layers.LSTM(LSTM_DIM, return_sequences=False))\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(4, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 1000, 300)         12036900  \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 1000, 300)         0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               219648    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 4)                 1028      \n",
            "=================================================================\n",
            "Total params: 12,290,600\n",
            "Trainable params: 12,290,600\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgd-4oUgtf9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9cf2270-8bd2-4c00-c383-dcc2f2459df3"
      },
      "source": [
        "batch_size = 100\n",
        "model.fit(X_train, to_categorical(train_sentiments), epochs=10, batch_size=batch_size, \n",
        "          shuffle=True, validation_split=0.1, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "69/69 [==============================] - 16s 235ms/step - loss: 0.0560 - accuracy: 0.9870 - val_loss: 0.2220 - val_accuracy: 0.9305\n",
            "Epoch 2/10\n",
            "69/69 [==============================] - 16s 234ms/step - loss: 0.0232 - accuracy: 0.9929 - val_loss: 0.1668 - val_accuracy: 0.9567\n",
            "Epoch 3/10\n",
            "69/69 [==============================] - 16s 235ms/step - loss: 0.0101 - accuracy: 0.9958 - val_loss: 0.1667 - val_accuracy: 0.9541\n",
            "Epoch 4/10\n",
            "69/69 [==============================] - 16s 235ms/step - loss: 0.0075 - accuracy: 0.9966 - val_loss: 0.1706 - val_accuracy: 0.9620\n",
            "Epoch 5/10\n",
            "69/69 [==============================] - 16s 236ms/step - loss: 0.0066 - accuracy: 0.9971 - val_loss: 0.1904 - val_accuracy: 0.9607\n",
            "Epoch 6/10\n",
            "69/69 [==============================] - 16s 235ms/step - loss: 0.0054 - accuracy: 0.9972 - val_loss: 0.1847 - val_accuracy: 0.9594\n",
            "Epoch 7/10\n",
            "69/69 [==============================] - 16s 235ms/step - loss: 0.0053 - accuracy: 0.9969 - val_loss: 0.1697 - val_accuracy: 0.9633\n",
            "Epoch 8/10\n",
            "69/69 [==============================] - 16s 234ms/step - loss: 0.0060 - accuracy: 0.9966 - val_loss: 0.1843 - val_accuracy: 0.9541\n",
            "Epoch 9/10\n",
            "69/69 [==============================] - 16s 234ms/step - loss: 0.0049 - accuracy: 0.9977 - val_loss: 0.1826 - val_accuracy: 0.9581\n",
            "Epoch 10/10\n",
            "69/69 [==============================] - 16s 236ms/step - loss: 0.0096 - accuracy: 0.9969 - val_loss: 0.1598 - val_accuracy: 0.9567\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f602beeb5d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOWeBj-BuL4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca8d4d82-c9a7-48a1-8651-a30e5a81ee2c"
      },
      "source": [
        "train_sequences[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[128,\n",
              " 5069,\n",
              " 8,\n",
              " 2,\n",
              " 362,\n",
              " 185,\n",
              " 168,\n",
              " 44,\n",
              " 1910,\n",
              " 11116,\n",
              " 6851,\n",
              " 13107,\n",
              " 4,\n",
              " 1910,\n",
              " 3464,\n",
              " 6851,\n",
              " 13108]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QsZg60KuL7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "774def1f-55be-4f90-9704-86a430f60639"
      },
      "source": [
        "X_train[1]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,   128,  5069,     8,     2,   362,   185,   168,\n",
              "          44,  1910, 11116,  6851, 13107,     4,  1910,  3464,  6851,\n",
              "       13108], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEOdNIHMtgAs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ebaf446-4ee8-4969-f33c-fe2f28f36e42"
      },
      "source": [
        "predictions = model.predict_classes(X_test)\n",
        "predictions[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 1, 0, 1, 1, 1, 2, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3Cpj6LCtgEI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9NmMYMwtgHp"
      },
      "source": [
        "submission = pd.read_excel('/content/Sample_submission.xlsx')\n",
        "submission['SECTION'] = predictions\n",
        "submission.to_excel('/content/Sample_submission_w2v_LSTM.xlsx',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "757Y_aH0tgLl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}